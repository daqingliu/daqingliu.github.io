<!DOCTYPE html>
<html lang="en-US">
  <head>
    <title>Daqing Liu | JD Explore Academy</title>

    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="This is the academic homepage of Dr. Daqing Liu.">
    
    <meta name="keywords" content="Daqing Liu, ÂàòÂ§ßÂ∫Ü, JDEA, USTC, multi-modality learning, vision and langauge, representaion, alignment, geeration, decision-making">
    
    
    <link rel="canonical" href="https://daqingliu.github.io/"/>
    

    <link rel="icon" media="(prefers-color-scheme:dark)" href="" type="image/png" />
    <link rel="icon" media="(prefers-color-scheme:light)" href="./assets/img/favicon.ico" type="image/png" />
    <script src="./assets/js/favicon-switcher.js" type="application/javascript"></script>

    <link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous>
    <link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin=anonymous>

    <link rel="stylesheet" href="./assets/css/style.css">
    <link rel="stylesheet" href="./assets/css/publications.css">
    
    <script type="text/javascript">
      function toggle_vis(id) {
          var e = document.getElementById(id);
          if (e.style.display == 'none')
              e.style.display = 'inline';
          else
              e.style.display = 'none';
      }
   </script>

  </head>
  <body>
    
    <lightonly>
      <div id="headcolor">
        <div id="colorheadspacer"></div>
      </div>
    </lightonly>

    <script src="./assets/js/back-to-top.js"></script>
    <script>addBackToTop({
        backgroundColor: '#fff',
        innerHTML: 'Back to Top',
        textColor: '#333'
      })
    </script><div id="back-to-top" class="hidden">Back to Top</div>
    <style>
        #back-to-top {
          border: 1px solid #ccc;
          border-radius: 0;
          font-size: 12px;
          width: 80px;
          text-align: center;
          line-height: 30px;
          height: 30px;
        }
    </style>

    <div class="wrapper">
      <header>
        
        
        <a class="image avatar"><img src="./assets/img/avatar.jpg" alt="avatar" /></a>
        

        <h1>Daqing Liu</h1>
        
        
        <position style="font-size:1.5rem; color:#036; font-family:STKaiti;"><b>ÂàòÂ§ßÂ∫Ü</b></position>
        <br>
        

        
        <position style="font-size:1.10rem;">Research Scientist</position>
        <br>
        
        
        <a href="" rel="noopener"><autocolor>JD Explore Academy</autocolor></a>
        <br>
        
        
        <email>liudq.ustc (at) gmail.com</email>
        

        <br>
        <br>
        <div class="social-icons">
        
        <a style="margin: 0 0 0 0" href="assets/files/CV_DaqingLiu_2303.pdf" target="_blank">
          <i class="ai ai-cv" style="font-size:1.3rem;"></i>
        </a>
        

        
        <a style="margin: 0 0 0 0" href="https://scholar.google.com/citations?hl=en&user=TbBfOVEAAAAJ" target="_blank">
          <i class="ai ai-google-scholar" style="font-size:1.3rem"></i>
        </a>
        

        
        <a style="margin: 0 0 0 0" href="https://github.com/daqingliu" target="_blank">
          <i class="fab fa-github" style="font-size:1.3rem;"></i>
        </a>
        

        <br>

        
        <a style="margin: 0 0 0 0" href="https://dblp.org/pid/225/5519" target="_blank">
          <i class="ai ai-dblp ai-fw" style="font-size:1.3rem;"></i>
        </a>
        

        
        <a style="margin: 0 0 0 0" href="https://orcid.org/0000-0002-8286-0105" target="_blank">
          <i class="ai ai-orcid-square ai-fw" style="font-size:1.3rem;"></i>
        </a>
        

        

        
        <a style="margin: 0 0 0 0" href="https://twitter.com/liudq_ustc" target="_blank">
          <i class="fab fa-twitter" style="font-size:1.3rem;"></i>
        </a>
        
        </div>
        <br>

      </header>
      <section>

      <h2 id="about-me">About Me</h2>

<p>Currently, I am a research scientist at JD Explore Academy (JDEA), JD.com Inc, led by Dr. <a href="https://scholar.google.com/citations?user=W5WbqgoAAAAJ&amp;hl=en" target="_blank">Xiaodong He</a> and Prof. <a href="https://scholar.google.com/citations?user=RwlJNLcAAAAJ&amp;hl=en" target="_blank">Dacheng Tao</a>. Before that, I received my Ph.D. degree from the University of Science and Technology of China (USTC) in 2021, advised by <a href="https://dblp.org/pers/hd/z/Zha:Zheng=Jun" target="_blank">Prof. Zheng-Jun Zha</a>.
<!-- Additionally, I had the opportunity to be a visiting student at Nanyang Technological University (NTU) from May 2018 to May 2019, collaborating closely with <a href="http://www.ntu.edu.sg/home/hanwangzhang/" target="_blank">Associate Prof. Hanwang Zhang</a>. -->
From May 2018 to May 2019, I was a visiting student at Nanyang Technological University (NTU), working with <a href="http://www.ntu.edu.sg/home/hanwangzhang/" target="_blank">Associate Prof. Hanwang Zhang</a>.</p>

<h2 id="research-interests">Research Interests</h2>

<p style="margin: 0 0 5px 0">My research revolves around the exciting field of <b>Multi-Modality Learning (MML)</b> and its practical applications. I am driven by the ambition to develop an artificial general intelligence (AGI) that can effectively represent and comprehend information from diverse modalities, enabling it to make informed decisions. Within the realm of MML, I aim to bridge the gap between different modalities, unlock the full potential of multimodal data, and explore various research areas that contribute to this overarching goal, including:</p>

<ul>
  <li><b>Multi-Modality Representation</b>
    <ul style="margin: 0 0 5px 0">
      <li>Image self-supervised learning (<a href="https://arxiv.org/abs/2206.10207" target="_blank">SemMAE[NeurIPS'22]</a>)</li>
      <li>Video representation learning (<a href="#">VSP[CVPR'23]</a>)</li>
    </ul>
  </li>
  <li><b>Multi-Modality Alignment</b>
    <ul style="margin: 0 0 5px 0">
      <li>Visual grounding (<a href="https://arxiv.org/abs/1812.03299" target="_blank">NMTree[ICCV'19]</a>, <a href="https://arxiv.org/abs/1906.01784" target="_blank">RvG-Tree[TPAMI'21]</a>, <a href="https://arxiv.org/abs/2206.06619" target="_blank">TransVG++[TPAMI'23]</a>)</li>
      <li>Cross-modal retrieval (<a href="#">TCP[ICCV'23]</a>, <a href="#">CMI[arXiv'23]</a>)</li>
    </ul>
  </li>
  <li><b>Multi-Modality Generation</b>
    <ul style="margin: 0 0 5px 0">
      <li>Visual captioning (<a href="https://arxiv.org/abs/1906.02365" target="_blank">CAVP[MM'18, TPAMI'21]</a>, <a href="https://arxiv.org/abs/2004.00390" target="_blank">POS-SCAN[CVPR'20]</a>, <a href="https://arxiv.org/abs/2007.09049" target="_blank">RMN[IJCAI'20]</a>, <a href="https://arxiv.org/abs/2201.01984" target="_blank">CBT[arXiv'22]</a>, <a href="#">Prefix-Captioning[arXiv'23]</a>)</li>
      <li>Image generation (<a href="https://arxiv.org/abs/2206.00923" target="_blank">TwFA[CVPR'22]</a>, <a href="https://arxiv.org/abs/2302.02394">DualCycleDiffsion[TCSVT'23]</a>, <a href="https://jabir-zheng.github.io/MMoT/">MMoT[arXiv'23]</a>), <a href="https://mhh0318.github.io/cocktail/">Cocktailüç∏[arXiv'23]</a>)</li>
    </ul>
  </li>
  <li><b>Multi-Modality Decision-Making</b>
    <ul style="margin: 0 0 5px 0">
      <li>Visual question answering (<a href="https://arxiv.org/abs/2211.11190" target="_blank">CMCL[arXiv'22]</a>)</li>
      <li>Vision-language navigation (<a href="https://arxiv.org/abs/2303.01032" target="_blank">ESceme[arXiv'23]</a>)</li>
    </ul>
  </li>
</ul>

<h2 id="news">News</h2>

<ul>
  <li><strong>[2023-07-14]</strong> üåüüåü One paper about cross-modal retrieval is accepted to <a href="https://iccv2023.thecvf.com/" target="_blank">ICCV 2023</a>!</li>
  <li><strong>[2023-07-06]</strong> üåüüåü One paper about visual grounding is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34" target="_blank">IEEE TPAMI</a>!</li>
  <li><strong>[2023-06-12]</strong> One paper about image editing is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76" target="_blank">IEEE TCSVT</a>.</li>
  <li><strong>[2023-06-01]</strong> üî•üî• Our AIGC project <a href="https://mhh0318.github.io/cocktail/" target="_blank">Cocktailüç∏</a> is avaliable!</li>
  <li><strong>[2023-05-10]</strong> üî•üî• Our AIGC project <a href="https://jabir-zheng.github.io/MMoT/" target="_blank">MMoT</a> is avaliable!</li>
  <li><strong>[2023-03-22]</strong> üéñüéñ Our CVPR paper VSP is selected as highlight (top 2.5%)!</li>
  <li><strong>[2023-02-28]</strong> One paper about video representation learning is accepted to <a href="https://cvpr2023.thecvf.com/" target="_blank">CVPR 2023</a>.</li>
  <li><strong>[2022-09-15]</strong> One paper about image self-supervised learning is accepted to <a href="https://neurips.cc/Conferences/2022" target="_blank">NeurIPS 2022</a>.</li>
  <li><strong>[2022-06-30]</strong> One paper about image deblurring is accepted to <a href="https://2022.acmmm.org/" target="_blank">ACM MM 2022</a>.</li>
  <li><strong>[2022-03-03]</strong> One paper about image generation is accepted to <a href="https://cvpr2022.thecvf.com/" target="_blank">CVPR 2022</a>.</li>

<li> <a href="javascript:toggle_vis('newsmore')">Show more</a> </li>
<div id="newsmore" style="display:none"> 
  <li><strong>[2022-03-01]</strong> I finished my rotation and joint JD Explore Academy, as a research scientist.</li>
  <li><strong>[2021-08-05]</strong> I joint JD.com Inc., as a <a href="https://campus.jd.com/web/static/forward?to=jd-project-dmt&amp;t=3" target="_blank">Doctoral Management Trainee</a>.</li>
  <li><strong>[2021-05-22]</strong> üéâüéâ I successfully defended my PhD thesis!</li>
  <li><strong>[2020-04-20]</strong> One paper about visual captioning is accepted to <a href="https://ijcai20.org/" target="_blank">IJCAI 2020</a>.</li>
  <li><strong>[2020-02-27]</strong> One paper about visual captioning is accepted to <a href="http://cvpr2020.thecvf.com/">CVPR 2020</a>.</li>
  <li><strong>[2019-07-23]</strong> One paper about visual grounding is accepted to <a href="http://iccv2019.thecvf.com/">ICCV 2019</a> as Oral.</li>
  <li><strong>[2019-03-27]</strong> Two papers about visual captioning and visual grounding are accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">TPAMI</a>.</li>
  <li><strong>[2018-07-02]</strong> One paper about visual captioning is accepted to <a href="https://dl.acm.org/doi/proceedings/10.1145/3240508">ACM MM 2018</a> as Oral.</li>
  <li><strong>[2016-03-05]</strong> üéâüéâ I start my research journey from USTC!</li>
</div>

</ul>

<h2 id="publications" style="margin: 2px 0px -15px;">Publications</h2>

<div class="publications">

<span class="superscript">*</span>: Co-First Authors, <span class="superscript">#</span>: Supervised Students, <span class="superscript">&#9993;</span>: Corresponding Authors
<br />
I marked my name in <b style="color:#a82e26">Red</b> if I am the first or corresponding author, or the mentor of the first author.
<br /><br />
<ol class="bibliography">

<!-- d ----------------------- -->
<h3 class="bibliography-year" style="font-size: 1.3rem; padding-bottom:5px; border-bottom: 1px solid #ddd;"> arXiv </h3>

<!-- d ----------------------- 
<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;"> <abbr class="badge" style="width:40px; align:middle">arXiv</abbr> </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="" target="_blank">MMoT: Mixture-of-Modality-Tokens Transformer for Composed Multimodal Conditional Image Synthesis</a></div>
    <div class="author">Jianbin Zheng<span class="superscript">#</span>, <b style="color:#a82e26">Daqing Liu</b>, Chaoyue Wang, Minghui Hu, Changxing Ding, Dacheng Tao</div>
    <div class="periodical"><em>arXiv preprint, 2023.</em></div>
    <div class="links">
      <a href="" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Coming Soon</a>
    </div>
  </div>
</div>
</li>
-->

<!-- d  ----------------------- 
<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;"> <abbr class="badge" style="width:40px; align:middle">arXiv</abbr> </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="" target="_blank">Tackling the Cross-Modal Retrieval Trilemma with Cross-Modal Indexing</a></div>
    <div class="author">Heng Zhang<span class="superscript">#</span>, <b style="color:#a82e26">Daqing Liu</b>, Heliang Zheng, Chaoyue Wang, Bing Su</div>
    <div class="periodical"><em>arXiv preprint, 2023.</em></div>
    <div class="links">
      <a href="" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Coming Soon</a>
    </div>
  </div>
</div>
</li>
-->

<!-- d ----------------------- 
<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;"> <abbr class="badge" style="width:40px; align:middle">arXiv</abbr> </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="" target="_blank">Prefix-Captioning: Efficiently Gluing Pretrained Language and Vision Models for Image Captioning</a></div>
    <div class="author">Yuanen Zhou<span class="superscript">#</span>, <b style="color:#a82e26">Daqing Liu</b>, Zhenzhen Hu, Depeng Wang, Yi Wang, Meng Wang</div>
    <div class="periodical"><em>arXiv preprint, 2023.</em></div>
    <div class="links">
      <a href="" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Coming Soon</a>
    </div>
  </div>
</div>
</li>
-->

<!-- d ----------------------- -->
<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;"> <abbr class="badge" style="width:40px; align:middle">arXiv</abbr> </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://arxiv.org/abs/2303.01032" target="_blank">Cocktailüç∏: Mixing Multi-Modality Controls for Text-Conditional Image Generation</a></div>
    <div class="author">Minghui Hu<span class="superscript">#</span>, Jianbin Zheng<span class="superscript">#</span>, <b style="color:#a82e26">Daqing Liu</b>, Chuanxia Zheng, Chaoyue Wang, Dacheng Tao, Tat-Jen Cham</div>
    <div class="periodical"><em>arXiv preprint, arXiv:2306.00964.</em></div>
    <div class="links">
      <a href="https://arxiv.org/abs/2306.00964" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">arXiv</a>
      <a href="https://arxiv.org/pdf/2306.00964" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://mhh0318.github.io/cocktail/" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Project Page</a>
      <a href="https://github.com/mhh0318/Cocktail" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      </div>
  </div>
</div>
</li>

<!-- d ----------------------- -->
<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;"> <abbr class="badge" style="width:40px; align:middle">arXiv</abbr> </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://arxiv.org/abs/2303.01032" target="_blank">MMoT: Mixture-of-Modality-Tokens Transformer for Composed Multimodal Conditional Image Synthesis</a></div>
    <div class="author">Jianbin Zheng<span class="superscript">#</span>, <b style="color:#a82e26">Daqing Liu<span class="superscript">*</span></b>, Chaoyue Wang, Minghui Hu, Zuopeng Yang, Changxing Ding, Dacheng Tao</div>
    <div class="periodical"><em>arXiv preprint, arXiv:2305.05992.</em></div>
    <div class="links">
      <a href="https://arxiv.org/abs/2305.05992" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">arXiv</a>
      <a href="https://arxiv.org/pdf/2305.05992" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://jabir-zheng.github.io/MMoT/" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Project Page</a>
      <a href="https://github.com/jabir-zheng/MMoT-Transformer" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      </div>
  </div>
</div>
</li>

<!-- d ----------------------- -->
<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;"> <abbr class="badge" style="width:40px; align:middle">arXiv</abbr> </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://arxiv.org/abs/2303.01032" target="_blank">ESceme: Vision-and-Language Navigation with Episodic Scene Memory</a></div>
    <div class="author">Qi Zheng<span class="superscript">#</span>, <b style="color:#a82e26">Daqing Liu</b>, Chaoyue Wang, Jing Zhang, Dadong Wang, Dacheng Tao</div>
    <div class="periodical"><em>arXiv preprint, arXiv:2303.01032.</em></div>
    <div class="links">
      <a href="https://arxiv.org/abs/2303.01032" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">arXiv</a>
      <a href="https://arxiv.org/pdf/2303.01032" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/qizhust/esceme" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
    </div>
  </div>
</div>
</li>

<!-- d ----------------------- -->
<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;"> <abbr class="badge" style="width:40px; align:middle">arXiv</abbr> </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://arxiv.org/abs/2303.00501" target="_blank">OmniForce: On Human-Centered, Large Model Empowered and Cloud-Edge Collaborative AutoML System</a></div>
    <div class="author">Chao Xue, Wei Liu, Shuai Xie, Zhenfang Wang, Jiaxing Li, Xuyang Peng, Liang Ding, Shanshan Zhao, Qiong Cao, Yibo Yang, Fengxiang He, Bohua Cai, Rongcheng Bian, Yiyan Zhao, Heliang Zheng, Xiangyang Liu, Dongkai Liu, <b>Daqing Liu</b>, Li Shen, Chang Li, Shijin Zhang, Yukang Zhang, Guanpu Chen, Shixiang Chen, Yibing Zhan, Jing Zhang, Chaoyue Wang, Dacheng Tao</div>
    <div class="periodical"><em>arXiv preprint, arXiv:2303.00501.</em></div>
    <div class="links">
      <a href="https://arxiv.org/abs/2303.00501" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">arXiv</a>
      <a href="https://arxiv.org/pdf/2303.00501" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <i>&nbsp;&nbsp;Journal Version, technical report of our OmniForce System</i>
    </div>
  </div>
</div>
</li>

<!-- d ----------------------- -->
<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;"> <abbr class="badge" style="width:40px; align:middle">arXiv</abbr> </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
  <div class="title"><a href="https://arxiv.org/abs/2211.11190" target="_blank">Cross-Modal Contrastive Learning for Robust Reasoning in VQA</a></div>
  <div class="author">Qi Zheng<span class="superscript">#</span>, Chaoyue Wang, <b style="color:#a82e26">Daqing Liu</b>, Dadong Wang, Dacheng Tao</div>
  <div class="periodical"><em>arXiv preprint, arXiv:2211.11190.</em></div>
    <div class="links">
      <a href="https://arxiv.org/abs/2211.11190" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">arXiv</a>
      <a href="https://arxiv.org/pdf/2211.11190" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/qizhust/cmcl_vqa_pl" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      <i>&nbsp;&nbsp;Journal Version</i>
    </div>
  </div>
</div>
</li>

<!-- d ----------------------- -->
<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;"> <abbr class="badge" style="width:40px; align:middle">arXiv</abbr> </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
  <div class="title"><a href="https://arxiv.org/abs/2201.01984" target="_blank">Compact Bidirectional Transformer for Image Captioning</a></div>
  <div class="author">Yuanen Zhou<span class="superscript">#</span>, Zhenzhen Hu, <b style="color:#a82e26">Daqing Liu</b>, Huixia Ben, Meng Wang</div>
  <div class="periodical"><em>arXiv preprint, arXiv:2201.01984</em></div>
    <div class="links">
      <a href="https://arxiv.org/abs/2201.01984" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">arXiv</a>
      <a href="https://arxiv.org/pdf/2201.01984" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/yuanezhou/cbtrans" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
    </div>
  </div>
</div>
</li>

<!-- d ----------------------- -->
<h3 class="bibliography-year" style="font-size: 1.3rem; padding-bottom:5px; border-bottom: 1px solid #ddd;"> 2023 </h3>

<!-- d ----------------------- -->
<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;"> <abbr class="badge" style="width:40px; align:middle">ICCV</abbr> </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="">Exploring Temporal Concurrency for Video-Language Representation Learning</a></div>
    <div class="author">Heng Zhang<span class="superscript">#</span>, <b style="color:#a82e26">Daqing Liu</b>, Bing Su, Dacheng Tao</div>
    <div class="periodical"><em>IEEE/CVF International Conference on Computer Vision <strong>(ICCV)</strong>, 2023.</em></div>
    <div class="links">
      <a href="" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Coming soon</a>
      <!-- <a href="" role="button" target="_blank" style="font-size:12px;">Code</a> -->
      <!-- <strong><i style="color:#e74d3c">&nbsp;&nbsp;Highlight, Accept Rate: 2.5%</i></strong> -->
    </div>
  </div>
</div>
</li>

<!-- d ----------------------- -->
<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;"> <abbr class="badge" style="width:40px; align:middle">TPAMI</abbr> </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
  <div class="title"><a href="https://arxiv.org/abs/2206.06619" target="_blank">TransVG++: End-to-End Visual Grounding with Language Conditioned Vision Transformer</a></div>
  <div class="author">Jiajun Deng, Zhengyuan Yang, <b>Daqing Liu</b>, Tianlang Chen, Wengang Zhou, Yanyong Zhang, Houqiang Li, Wanli Ouyang</div>
  <div class="periodical"><em>IEEE Transactions on Pattern Analysis and Machine Intelligence.</em></div>
    <div class="links">
      <a href="https://arxiv.org/abs/2206.06619" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">arXiv</a>
      <a href="https://arxiv.org/pdf/2206.06619" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/djiajunustc/TransVG" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      <strong><i style="color:#e74d3c">&nbsp;&nbsp;Impact Factor: 24.314</i></strong>
    </div>
  </div>
</div>
</li>

<!-- d ----------------------- -->
<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;"> <abbr class="badge" style="width:40px; align:middle">TCSVT</abbr> </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
  <div class="title"><a href="https://arxiv.org/abs/2302.02394" target="_blank">Eliminating Prior Bias for Semantic Image Editing via Dual-Cycle Diffusion</a></div>
  <div class="author">Zuopeng Yang<span class="superscript">#</span>, Tianshu Chu, Xin Lin, Erdun Gao, <b style="color:#a82e26">Daqing Liu</b>, Jie Yang, Chaoyue Wang</div>
  <div class="periodical"><em>IEEE Transactions on Circuits and Systems for Video Technology.</em></div>
    <div class="links">
      <a href="https://arxiv.org/abs/2302.02394" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">arXiv</a>
      <a href="https://arxiv.org/pdf/2302.02394" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/JohnDreamer/DualCycleDiffsion" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
    </div>
  </div>
</div>
</li>

<!-- d ----------------------- -->
<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;"> <abbr class="badge" style="width:40px; align:middle">CVPR</abbr> </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Modeling_Video_As_Stochastic_Processes_for_Fine-Grained_Video_Representation_Learning_CVPR_2023_paper.pdf">Modeling Video as Stochastic Processes for Fine-Grained Video Representation Learning</a></div>
    <div class="author">Heng Zhang<span class="superscript">#</span>, <b style="color:#a82e26">Daqing Liu<span class="superscript">*</span></b>, Qi Zheng, Bing Su</div>
    <div class="periodical"><em>IEEE/CVF Conference on Computer Vision and Pattern Recognition <strong>(CVPR)</strong>, 2023.</em></div>
    <div class="links">
      <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Modeling_Video_As_Stochastic_Processes_for_Fine-Grained_Video_Representation_Learning_CVPR_2023_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://www.youtube.com/watch?v=ANfWcISTPK8&amp;ab_channel=zhangheng" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      <strong><i style="color:#e74d3c">&nbsp;&nbsp;Highlight, Accept Rate: 2.5%</i></strong>
    </div>
  </div>
</div>
</li>

<!-- d ----------------------- -->
<h3 class="bibliography-year" style="font-size: 1.3rem; padding-bottom:5px; border-bottom: 1px solid #ddd;"> 2022 </h3>

<!-- d ----------------------- -->
<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;"> <abbr class="badge" style="width:40px; align:middle">NeurIPS</abbr> </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
  <div class="title"><a href="https://openreview.net/forum?id=Ix37FJYDkBp" target="_blank">SemMAE: Semantic-Guided Masking for Learning Masked Autoencoders</a></div>
  <div class="author">Gang Li, Heliang Zheng, <b>Daqing Liu</b>, Bing Su, Changwen Zheng</div>
  <div class="periodical"><em>Neural Information Processing Systems <strong>(NeurIPS)</strong>, 2022.</em></div>
    <div class="links">
      <a href="https://arxiv.org/abs/2206.10207" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">arXiv</a>
      <a href="https://arxiv.org/pdf/2206.10207" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/ucasligang/SemMAE" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      <i>Full, Accept Rate: 25.6%</i>
    </div>
  </div>
</div>
</li>

<!-- d ----------------------- -->
<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;"> <abbr class="badge" style="width:40px; align:middle">MM</abbr> </div>
    <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548106" target="_blank">Semantically-Consistent Dynamic Blurry Image Generation for Image Deblurring</a></div>
    <div class="author">Zhaohui Jing, Youjian Zhang, Chaoyue Wang, <b>Daqing Liu</b>, Yong Xia</div>
    <div class="periodical"><em>ACM International Conference on Multimedia <strong>(ACM MM)</strong>, 2022.</em></div>
    <div class="links">
      <i>Full, Accept Rate: 27.9%</i>
    </div>
  </div>
</div>
</li>

<!-- d ----------------------- -->
<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;"> <abbr class="badge" style="width:40px; align:middle">CVPR</abbr> </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
  <div class="title"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Yang_Modeling_Image_Composition_for_Complex_Scene_Generation_CVPR_2022_paper" target="_blank">Modeling Image Composition for Complex Scene Generation</a></div>
  <div class="author">Zuopeng Yang<span class="superscript">#</span>, <b style="color:#a82e26">Daqing Liu<span class="superscript">*</span></b>, Chaoyue Wang, Jie Yang, Dacheng Tao</div>
    <div class="periodical"><em>IEEE/CVF Conference on Computer Vision and Pattern Recognition <strong>(CVPR)</strong>, 2022.</em></div>
    <div class="links">
      <a href="https://arxiv.org/abs/2206.00923" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">arXiv</a>
      <a href="https://arxiv.org/pdf/2206.00923" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/JohnDreamer/TwFA" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      <i>Full, Accept Rate: 25.3%</i>
    </div>
  </div>
</div>
</li>

<!-- d ----------------------- -->
<h3 class="bibliography-year" style="font-size: 1.3rem; padding-bottom:5px; border-bottom: 1px solid #ddd;"> 2021 </h3>

<!-- d ----------------------- -->
<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;"> <abbr class="badge" style="width:40px; align:middle">TPAMI</abbr> </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://ieeexplore.ieee.org/document/8684270/" target="_blank">Context-Aware Visual Policy Network for Fine-Grained Image Captioning</a></div>
    <div class="author">Zheng-Jun Zha, <b style="color:#a82e26">Daqing Liu<span class="superscript">&#9993;</span></b>, Hanwang Zhang, Yongdong Zhang, Feng Wu</div>
    <div class="periodical"><em>IEEE Transactions on Pattern Analysis and Machine Intelligence.</em></div>
    <div class="links">
      <a href="https://arxiv.org/abs/1906.02365" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">arXiv</a>
      <a href="https://arxiv.org/pdf/1906.02365" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/daqingliu/CAVP" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      <strong><i style="color:#e74d3c">&nbsp;&nbsp;Impact Factor: 24.314</i></strong>
    </div>
  </div>
</div>
</li>

<!-- d ----------------------- -->
<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;"> <abbr class="badge" style="width:40px; align:middle">TPAMI</abbr> </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://ieeexplore.ieee.org/document/8691415" target="_blank">Learning to Compose and Reason with Language Tree Structures for Visual Grounding</a></div>
    <div class="author">Richang Hong, <b style="color:#a82e26">Daqing Liu<span class="superscript">&#9993;</span></b>, Xiaoyu Mo, Xiangnan He, Hanwang Zhang</div>
    <div class="periodical"><em>IEEE Transactions on Pattern Analysis and Machine Intelligence.</em></div>
    <div class="links">
      <a href="https://arxiv.org/abs/1906.01784" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">arXiv</a>
      <a href="https://arxiv.org/pdf/1906.01784" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <strong><i style="color:#e74d3c">&nbsp;&nbsp;Impact Factor: 24.314</i></strong>
    </div>
  </div>
</div>
</li>

<!-- d ----------------------- -->
<h3 class="bibliography-year" style="font-size: 1.3rem; padding-bottom:5px; border-bottom: 1px solid #ddd;"> 2020 </h3>
<!-- d ----------------------- -->
<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;"> <abbr class="badge" style="width:40px; align:middle">IJCAI</abbr> </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://www.ijcai.org/proceedings/2020/0104.pdf" target="_blank">Learning to Discretely Compose Reasoning Module Networks for Video Captioning</a></div>
    <div class="author">Ganchao Tan<span class="superscript">#</span>, <b style="color:#a82e26">Daqing Liu<span class="superscript">*</span></b>, Meng Wang, Zheng-Jun Zha </div>
    <div class="periodical"><em>International Joint Conference on Artificial Intelligence <strong>(IJCAI)</strong>, 2020.</em></div>
    <div class="links">
      <a href="https://arxiv.org/abs/2007.09049" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">arXiv</a>
      <a href="https://arxiv.org/pdf/2007.09049" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/tgc1997/RMN" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      <strong><i style="color:#e74d3c">Oral, Accept Rate: 12.6%</i></strong>
    </div>
  </div>
</div>
</li>

<!-- d ----------------------- -->
<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;"> <abbr class="badge" style="width:40px; align:middle">CVPR</abbr> </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Zhou_More_Grounded_Image_Captioning_by_Distilling_Image-Text_Matching_Model_CVPR_2020_paper" target="_blank">More Grounded Image Captioning by Distilling Image-Text Matching Model</a></div>
    <div class="author">Yuanen Zhou<span class="superscript">#</span>, Meng Wang, <b style="color:#a82e26">Daqing Liu</b>, Zhenzhen Hu, Hanwang Zhang</div>
    <div class="periodical"><em>IEEE/CVF Conference on Computer Vision and Pattern Recognition <strong>(CVPR)</strong>, 2020.</em></div>
    <div class="links">
      <a href="https://arxiv.org/abs/2004.00390" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">arXiv</a>
      <a href="https://arxiv.org/pdf/2004.00390" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/YuanEZhou/Grounded-Image-Captioning" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      <i>&nbsp;&nbsp;Full, Accept Rate: 22%</i>
    </div>
  </div>
</div>
</li>

<!-- d ----------------------- -->
<h3 class="bibliography-year" style="font-size: 1.3rem; padding-bottom:5px; border-bottom: 1px solid #ddd;"> 2019 </h3>

<!-- d ----------------------- 
<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;"> <abbr class="badge" style="width:40px; align:middle">arXiv</abbr> </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://arxiv.org/abs/1906.03561" target="_blank">Joint Visual Grounding with Language Scene Graphs</a></div>
    <div class="author"><b style="color:#a82e26">Daqing Liu</b>, Hanwang Zhang, Zheng-Jun Zha, Meng Wang, Qianru Sun</div>
    <div class="periodical"><em>arXiv preprint, 2019.</em></div>
    <div class="links">
      <a href="https://arxiv.org/abs/1906.03561" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">arXiv</a>
      <a href="https://arxiv.org/pdf/1906.03561" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
    </div>
  </div>
</div>
</li>
-->

<!-- d ----------------------- -->
<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;"> <abbr class="badge" style="width:40px; align:middle">ICCV</abbr> </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Learning_to_Assemble_Neural_Module_Tree_Networks_for_Visual_Grounding_ICCV_2019_paper" target="_blank">Learning to Assemble Neural Module Tree Networks for Visual Grounding</a></div>
    <div class="author"><b style="color:#a82e26">Daqing Liu</b>, Hanwang Zhang, Zheng-Jun Zha, Feng Wu</div>
    <div class="periodical"><em>IEEE/CVF International Conference on Computer Vision <strong>(ICCV)</strong>, 2019.</em></div>
    <div class="links">
      <a href="https://arxiv.org/abs/1812.03299" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">arXiv</a>
      <a href="https://arxiv.org/pdf/1812.03299" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/daqingliu/NMTree" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      <a href="https://youtu.be/oFDF1yT0T-4?t=3574" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Talk</a>
      <strong><i style="color:#e74d3c">&nbsp;&nbsp;Oral, Accept Rate: 4.3%</i></strong>
    </div>
  </div>
</div>
</li>

<!-- d ----------------------- -->
<h3 class="bibliography-year" style="font-size: 1.3rem; padding-bottom:5px; border-bottom: 1px solid #ddd;"> 2018 </h3>
<!-- d ----------------------- -->
<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;"> <abbr class="badge" style="width:40px; align:middle">MM</abbr> </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title"><a href="https://dl.acm.org/doi/10.1145/3240508.3240632" target="_blank">Context-Aware Visual Policy Network for Sequence-Level Image Captioning</a></div>
    <div class="author"><b style="color:#a82e26">Daqing Liu</b>, Zheng-Jun Zha, Hanwang Zhang, Yongdong Zhang, Feng Wu</div>
    <div class="periodical"><em>ACM International Conference on Multimedia <strong>(ACM MM)</strong>, 2018.</em></div>
    <div class="links">
      <a href="https://arxiv.org/abs/1808.05864" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">arXiv</a>
      <a href="https://arxiv.org/pdf/1808.05864" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/daqingliu/CAVP" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      <strong><i style="color:#e74d3c">&nbsp;&nbsp;Oral, Accept Rate: 8.5%</i></strong>
    </div>
  </div>
</div>
</li>

</ol>
</div>

<p></p>
<h2 id="experience">Experience</h2>

<h4 style="margin:0 10px 0;">Research Scientist (2021-08 -- Present)</h4>

<ul style="margin:0 0 5px;">
  JD Explore Academy (JDEA), JD.com, Beijing, China
  <br />
  Leader: Prof. <a href="https://scholar.google.com/citations?user=RwlJNLcAAAAJ&amp;hl=en" target="_blank">Dacheng Tao</a>
</ul>

<h4 style="margin:0 10px 0;">Research Intern (2018-05 -- 2019-05)</h4>

<ul style="margin:0 0 5px;">
  Nanyang Technological University (NTU), Singapore
  <br />
  Advisior: Associate Prof. <a href="http://www.ntu.edu.sg/home/hanwangzhang/" target="_blank">Hanwang Zhang</a>
</ul>

<h4 style="margin:0 10px 0;">Doctor of Engineering (2016-09 -- 2021-06)</h4>

<ul style="margin:0 0 5px;">
  University of Science and Technology of China (USTC), Hefei, China
  <br />
  Supervisor: Prof. <a href="https://dblp.org/pers/hd/z/Zha:Zheng=Jun" target="_blank">Zheng-Jun Zha</a>
</ul>
<p></p>
<h2 id="services">Services</h2>

<h4 style="margin:0 10px 0;">Conference Reviewers</h4>

<ul style="margin:0 0 5px;">
  <li><a href="http://cvpr2023.thecvf.com/" target="_blank"><autocolor>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2021-2023</autocolor></a></li>
  <li><a href="http://nips.cc/" target="_blank"><autocolor>IEEE/CVF International Conference on Computer Vision (ICCV) 2021, 2023</autocolor></a></li>
  <li><a href="http://iccv2023.thecvf.com/" target="_blank"><autocolor>Conference on Neural Information Processing Systems (NeurIPS) 2023</autocolor></a></li>
  <li><a href="https://eccv2022.ecva.net/" target="_blank"><autocolor>European Conference on Computer Vision (ECCV) 2020, 2022</autocolor></a></li>
  <li><a href="https://aaai-23.aaai.org/" target="_blank"><autocolor>AAAI Conference on Artificial Intelligence (AAAI) 2021-2023</autocolor></a></li>
  <li><a href="https://ijcai-23.org/" target="_blank"><autocolor>International Joint Conference on Artificial Intelligence (IJCAI) 2020-2023</autocolor></a></li>
</ul>

<h4 style="margin:0 10px 0;">Journal Reviewers</h4>

<ul style="margin:0 0 20px;">
  <li><a href="https://mc.manuscriptcentral.com/tip-ieee" target="_blank"><autocolor>IEEE Transactions on Image Processing (TIP)</autocolor></a></li>
  <li><a href="https://mc.manuscriptcentral.com/tmm-ieee" target="_blank"><autocolor>IEEE Transactions on Multimedia (TMM)</autocolor></a></li>
  <li><a href="https://mc.manuscriptcentral.com/tcsvt" target="_blank"><autocolor>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</autocolor></a></li>
  <li><a href="https://mc.manuscriptcentral.com/tnnls" target="_blank"><autocolor>IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</autocolor></a></li>
  <!-- <li><a href="https://mc.manuscriptcentral.com/cyb-ieee" target="_blank"><autocolor>IEEE Transactions on Cybernetics (CYB)</autocolor></a></li>
  <li><a href="https://www.editorialmanager.com/NEUCOM/" target="_blank"><autocolor>Neurocomputing (NEUCOM)</autocolor></a></li>
  <li><a href="https://www.editorialmanager.com/ncaa" target="_blank"><autocolor>Neural Computing and Applications (NCAA)</autocolor></a></li>
  <li><a href="https://mc03.manuscriptcentral.com/mir" target="_blank"><autocolor>IEEE Transactions on Machine Intelligence Research (MIR)</autocolor></a></li> -->
</ul>

<h2 id="contact">Contact</h2>
<p><strong>Address</strong>: <a href="https://goo.gl/maps/L8yR2Wv3D1DquqBS9" target="_blank">Block 2-A-13, JD Building, Beijing, China, 101111</a>
<br />
<strong>Phone</strong>: (86) 188-5698-8126 ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† <strong>Wechat</strong>: liudaqing94</p>
<p></p>


      <br>

      
      <p><small>Powered by Jekyll and <a href="https://github.com/yaoyao-liu/minimal-light" target="_blank" rel="noopener">Minimal Light</a> theme.</small></p>
      

      </section>
      <footer>
        What I cannot create,<br>
        I do not understand.
      </footer>
    </div>
    <script src="/assets/js/scale.fix.js"></script>
    
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
      ga('create', 'G-LTR2WQ5PNG', 'auto');
      ga('send', 'pageview');
    </script>
    
  </body>
</html>
