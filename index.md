---
layout: homepage
---

## About Me

Currently, I am a research scientist at JD Explore Academy (JDEA), JD.com Inc, leaded by Prof. <a href="https://scholar.google.com/citations?user=RwlJNLcAAAAJ&hl=en" target="_blank">Dacheng Tao</a>. Before that, I received my Ph.D. degree in Control Science and Engineering from the University of Science and Technology of China (USTC) in 2021, advised by <a href="https://dblp.org/pers/hd/z/Zha:Zheng=Jun" target="_blank">Prof. Zheng-Jun Zha</a>. From May 2018 to May 2019, I was a visiting student at Nanyang Technological University (NTU), working with <a href="http://www.ntu.edu.sg/home/hanwangzhang/" target="_blank">Asst. Prof. Hanwang Zhang</a>.

## Research Interests

<p  style="margin: 0 0 5px 0">My research interests are focused on <b>Multi-Modality Learning (MML)</b> and its practical applications, with the ultimate goal of developing an artifical general intelligence (AGI) that can represent and comprehend information from various modalities and then make informed decisions based on that information.</p>
<p style="margin: 0 0 2px 0"> Specifically, I am interested in the following areas:</p>
<ul>
  <li><b>Multi-Modality Representation</b>
    <ul style="margin: 0 0 5px 0">
      <li>Image self-supervised learning (<a href="https://arxiv.org/abs/2206.10207" target="_blank">SemMAE[NeurIPS'22]</a>)</li>
      <li>Video representation learning (<a href="#">VSP[CVPR'23]</a>)</li>
    </ul>
  </li>
  <li><b>Multi-Modality Alignment</b>
    <ul style="margin: 0 0 5px 0">
      <li>Visual grounding (<a href="https://arxiv.org/abs/1812.03299" target="_blank">NMTree[ICCV'19]</a>, <a href="https://arxiv.org/abs/1906.01784" target="_blank">RvG-Tree[TPAMI'21]</a>, <a href="https://arxiv.org/abs/2206.06619" target="_blank">TransVG++[arXiv'22]</a>)</li>
      <li>Cross-modal retrieval (<a href="#">CMI[arXiv'23]</a>)</li>
    </ul>
  </li>
  <li><b>Multi-Modality Generation</b>
    <ul style="margin: 0 0 5px 0">
      <li>Visual captioning (<a href="https://arxiv.org/abs/1906.02365" target="_blank">CAVP[MM'18, TPAMI'21]</a>, <a href="https://arxiv.org/abs/2004.00390" target="_blank">POS-SCAN[CVPR'20]</a>, <a href="https://arxiv.org/abs/2007.09049" target="_blank">RMN[IJCAI'20]</a>, <a href="https://arxiv.org/abs/2201.01984" target="_blank">CBT[arXiv'22]</a>, <a href="#">Prefix-Captioning[arXiv'23]</a>)</li>
      <li>Image generation (<a href="https://arxiv.org/abs/2206.00923" target="_blank">TwFA[CVPR'22]</a>, <a href="#">MMoT[arXiv'23]</a>)</li>
    </ul>
  </li>
  <li><b>Multi-Modality Decision-Making</b>
    <ul style="margin: 0 0 5px 0">
      <li>Visual question answering (<a href="https://arxiv.org/abs/2211.11190" target="_blank">CMCL[arXiv'22]</a>)</li>
      <li>Vision-language navigation (<a href="https://arxiv.org/abs/2303.01032" target="_blank">ESceme[arXiv'23]</a>)</li>
    </ul>
  </li>
</ul>


## News

<ul>
  <li><strong>[2023-03-22]</strong> ðŸŽ‰ðŸŽ‰ Our CVPR paper VSP is selected as highlight (top 2.5%)!</li>
  <li><strong>[2023-02-28]</strong> One paper about video representation learning is accepted to <a href="https://cvpr2023.thecvf.com/" target="_blank">CVPR 2023</a>.</li>
  <li><strong>[2022-09-15]</strong> One paper about image self-supervised learning is accepted to <a href="https://neurips.cc/Conferences/2022" target="_blank">NeurIPS 2022</a>.</li>
  <li><strong>[2022-06-30]</strong> One paper about image deblurring is accepted to <a href="https://2022.acmmm.org/" target="_blank">ACM MM 2022</a>.</li>
  <li><strong>[2022-03-03]</strong> One paper about image generation is accepted to <a href="https://cvpr2022.thecvf.com/" target="_blank">CVPR 2022</a>.</li>

<li> <a href="javascript:toggle_vis('newsmore')">Show more</a> </li>
<div id="newsmore" style="display:none"> 
  <li><strong>[2022-03-01]</strong> I finished my rotation and joint JD Explore Academy, as a research scientist.</li>
  <li><strong>[2021-08-05]</strong> I joint JD.com Inc., as a <a href="https://campus.jd.com/web/static/forward?to=jd-project-dmt&t=3" target="_blank">Doctoral Management Trainee</a>.</li>
  <li><strong>[2021-05-22]</strong> ðŸŽ‰ðŸŽ‰ I successfully defended my PhD thesis!</li>
  <li><strong>[2020-04-20]</strong> One paper about visual captioning is accepted to <a href="https://ijcai20.org/" target="_blank">IJCAI 2020</a>.</li>
  <li><strong>[2020-02-27]</strong> One paper about <a href="">visual captioning</a> is accepted to <a href="http://cvpr2020.thecvf.com/">CVPR 2020</a>.</li>
  <li><strong>[2019-07-23]</strong> One paper about <a href="">visual grounding</a> is accepted to <a href="http://iccv2019.thecvf.com/">ICCV 2019</a> as Oral.</li>
  <li><strong>[2019-03-27]</strong> Two papers about <a href="">visual captioning</a> and visual grounding are accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">TPAMI</a>.</li>
  <li><strong>[2018-07-02]</strong> One paper about <a href="">visual captioning</a> is accepted to <a href="https://dl.acm.org/doi/proceedings/10.1145/3240508">ACM MM 2018</a> as Oral.</li>
  <li><strong>[2016-03-05]</strong> ðŸŽ‰ðŸŽ‰ I start my research jourey from USTC!</li>
</div>

</ul>

{% include_relative _includes/publications.md %}
{% include_relative _includes/experience.md %}
{% include_relative _includes/services.md %}
