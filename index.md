---
layout: homepage
---

## About Me

Currently, I am a research scientist at JD Explore Academy (JDEA), JD.com Inc, led by Dr. <a href="https://scholar.google.com/citations?user=W5WbqgoAAAAJ&hl=en" target="_blank">Xiaodong He</a> and Prof. <a href="https://scholar.google.com/citations?user=RwlJNLcAAAAJ&hl=en" target="_blank">Dacheng Tao</a>. Before that, I received my Ph.D. degree from the University of Science and Technology of China (USTC) in 2021, advised by <a href="https://dblp.org/pers/hd/z/Zha:Zheng=Jun" target="_blank">Prof. Zheng-Jun Zha</a>.
<!-- Additionally, I had the opportunity to be a visiting student at Nanyang Technological University (NTU) from May 2018 to May 2019, collaborating closely with <a href="http://www.ntu.edu.sg/home/hanwangzhang/" target="_blank">Associate Prof. Hanwang Zhang</a>. -->
From May 2018 to May 2019, I was a visiting student at Nanyang Technological University (NTU), working with <a href="http://www.ntu.edu.sg/home/hanwangzhang/" target="_blank">Associate Prof. Hanwang Zhang</a>.

## Research Interests

<p style="margin: 0 0 5px 0">My research revolves around the exciting field of <b>Multi-Modality Learning (MML)</b> and its practical applications. I am driven by the ambition to develop an artificial general intelligence (AGI) that can effectively represent and comprehend information from diverse modalities, enabling it to make informed decisions. Within the realm of MML, I aim to bridge the gap between different modalities, unlock the full potential of multimodal data, and explore various research areas that contribute to this overarching goal, including:</p>

<ul>
  <li><b>Multi-Modality Representation</b>
    <ul style="margin: 0 0 5px 0">
      <li>Image self-supervised learning (<a href="https://arxiv.org/abs/2206.10207" target="_blank">SemMAE[NeurIPS'22]</a>)</li>
      <li>Video representation learning (<a href="#">VSP[CVPR'23]</a>)</li>
    </ul>
  </li>
  <li><b>Multi-Modality Alignment</b>
    <ul style="margin: 0 0 5px 0">
      <li>Visual grounding (<a href="https://arxiv.org/abs/1812.03299" target="_blank">NMTree[ICCV'19]</a>, <a href="https://arxiv.org/abs/1906.01784" target="_blank">RvG-Tree[TPAMI'21]</a>, <a href="https://arxiv.org/abs/2206.06619" target="_blank">TransVG++[TPAMI'23]</a>)</li>
      <li>Cross-modal retrieval (<a href="#">CMI[arXiv'23]</a>)</li>
    </ul>
  </li>
  <li><b>Multi-Modality Generation</b>
    <ul style="margin: 0 0 5px 0">
      <li>Visual captioning (<a href="https://arxiv.org/abs/1906.02365" target="_blank">CAVP[MM'18, TPAMI'21]</a>, <a href="https://arxiv.org/abs/2004.00390" target="_blank">POS-SCAN[CVPR'20]</a>, <a href="https://arxiv.org/abs/2007.09049" target="_blank">RMN[IJCAI'20]</a>, <a href="https://arxiv.org/abs/2201.01984" target="_blank">CBT[arXiv'22]</a>, <a href="#">Prefix-Captioning[arXiv'23]</a>)</li>
      <li>Image generation (<a href="https://arxiv.org/abs/2206.00923" target="_blank">TwFA[CVPR'22]</a>, <a href="https://arxiv.org/abs/2302.02394">DualCycleDiffsion[TCSVT'23]</a>, <a href="https://jabir-zheng.github.io/MMoT/">MMoT[arXiv'23]</a>), <a href="https://mhh0318.github.io/cocktail/">CocktailğŸ¸[arXiv'23]</a>)</li>
    </ul>
  </li>
  <li><b>Multi-Modality Decision-Making</b>
    <ul style="margin: 0 0 5px 0">
      <li>Visual question answering (<a href="https://arxiv.org/abs/2211.11190" target="_blank">CMCL[arXiv'22]</a>)</li>
      <li>Vision-language navigation (<a href="https://arxiv.org/abs/2303.01032" target="_blank">ESceme[arXiv'23]</a>)</li>
    </ul>
  </li>
</ul>


## News

<ul>
  <li><strong>[2023-07-06]</strong> ğŸŒŸğŸŒŸ One paper about visual grounding is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34" target="_blank">IEEE TPAMI</a>!</li>
  <li><strong>[2023-06-12]</strong> One paper about image editing is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76" target="_blank">IEEE TCSVT</a>.</li>
  <li><strong>[2023-06-01]</strong> ğŸ”¥ğŸ”¥ Our AIGC project <a href="https://mhh0318.github.io/cocktail/" target="_blank">CocktailğŸ¸</a> is avaliable!</li>
  <li><strong>[2023-05-10]</strong> ğŸ”¥ğŸ”¥ Our AIGC project <a href="https://jabir-zheng.github.io/MMoT/" target="_blank">MMoT</a> is avaliable!</li>
  <li><strong>[2023-03-22]</strong> ğŸ–ğŸ– Our CVPR paper VSP is selected as highlight (top 2.5%)!</li>
  <li><strong>[2023-02-28]</strong> One paper about video representation learning is accepted to <a href="https://cvpr2023.thecvf.com/" target="_blank">CVPR 2023</a>.</li>
  <li><strong>[2022-09-15]</strong> One paper about image self-supervised learning is accepted to <a href="https://neurips.cc/Conferences/2022" target="_blank">NeurIPS 2022</a>.</li>
  <li><strong>[2022-06-30]</strong> One paper about image deblurring is accepted to <a href="https://2022.acmmm.org/" target="_blank">ACM MM 2022</a>.</li>
  <li><strong>[2022-03-03]</strong> One paper about image generation is accepted to <a href="https://cvpr2022.thecvf.com/" target="_blank">CVPR 2022</a>.</li>

<li> <a href="javascript:toggle_vis('newsmore')">Show more</a> </li>
<div id="newsmore" style="display:none"> 
  <li><strong>[2022-03-01]</strong> I finished my rotation and joint JD Explore Academy, as a research scientist.</li>
  <li><strong>[2021-08-05]</strong> I joint JD.com Inc., as a <a href="https://campus.jd.com/web/static/forward?to=jd-project-dmt&t=3" target="_blank">Doctoral Management Trainee</a>.</li>
  <li><strong>[2021-05-22]</strong> ğŸ‰ğŸ‰ I successfully defended my PhD thesis!</li>
  <li><strong>[2020-04-20]</strong> One paper about visual captioning is accepted to <a href="https://ijcai20.org/" target="_blank">IJCAI 2020</a>.</li>
  <li><strong>[2020-02-27]</strong> One paper about visual captioning is accepted to <a href="http://cvpr2020.thecvf.com/">CVPR 2020</a>.</li>
  <li><strong>[2019-07-23]</strong> One paper about visual grounding is accepted to <a href="http://iccv2019.thecvf.com/">ICCV 2019</a> as Oral.</li>
  <li><strong>[2019-03-27]</strong> Two papers about visual captioning and visual grounding are accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">TPAMI</a>.</li>
  <li><strong>[2018-07-02]</strong> One paper about visual captioning is accepted to <a href="https://dl.acm.org/doi/proceedings/10.1145/3240508">ACM MM 2018</a> as Oral.</li>
  <li><strong>[2016-03-05]</strong> ğŸ‰ğŸ‰ I start my research journey from USTC!</li>
</div>

</ul>

{% include_relative _includes/publications.md %}
{% include_relative _includes/experience.md %}
{% include_relative _includes/services.md %}
