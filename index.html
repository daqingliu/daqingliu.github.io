<!DOCTYPE html>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta content="IE=7.0000" http-equiv="X-UA-Compatible">
<title>Daqing LIU - JD.com</title>
<meta name="description" content="Daqing Liu, 刘大庆, Research Scientist at JDEA. Multi-modality learning, vision and langauge, cross-modal retrieval, image generation.">
<meta name="keywords" content="Daqing Liu, 刘大庆, JDEA, USTC, multi-modality learning, vision and langauge, cross-modal retrieval, image generation">

<link rel="stylesheet" type="text/css" href="daqing.css">

<style>@-moz-keyframes nodeInserted{from{opacity:0.99;}to{opacity:1;}}@-webkit-keyframes nodeInserted{from{opacity:0.99;}to{opacity:1;}}@-o-keyframes nodeInserted{from{opacity:0.99;}to{opacity:1;}}@keyframes nodeInserted{from{opacity:0.99;}to{opacity:1;}}embed,object{animation-duration:.001s;-ms-animation-duration:.001s;-moz-animation-duration:.001s;-webkit-animation-duration:.001s;-o-animation-duration:.001s;animation-name:nodeInserted;-ms-animation-name:nodeInserted;-moz-animation-name:nodeInserted;-webkit-animation-name:nodeInserted;-o-animation-name:nodeInserted;}
</style></head>


<body>
<div id="content">

<!-- news ---------- -->
  <div id="news">
  <h2>News</h2><br>
  <font size="3px">

  <b>28 Feb 2023</b><br>
  <span class="easylink">
  One paper is accepted by <a href="https://cvpr2023.thecvf.com/" ,="" target="_blank">CVPR 2023</a>, about <b>video representation learning</b>.
  </span><br><br>

  <b>15 Sep 2022</b><br>
  <span class="easylink">
  One paper is accepted by <a href="https://nips.cc/" ,="" target="_blank">NeurIPS 2022</a>, about <b>masked image modeling</b>.
  </span><br><br>

  <b>30 Jun 2022</b><br>
  <span class="easylink">
  One paper is accepted by <a href="https://2022.acmmm.org/" ,="" target="_blank">ACM MM 2022</a>, about <b>image deblurring</b>.
  </span><br><br>

  <b>3 Mar 2022</b><br>
  <span class="easylink">
  One paper is accepted by <a href="https://cvpr2022.thecvf.com/" ,="" target="_blank">CVPR 2022</a>, about <b>layout to image</b>. 
  </span><br><br>

  <b>1 Mar 2022</b><br>
  <span class="easylink">
  I finished my rotation and joint JD Explore Academy, as a <b>research scientist</b>.
  </span><br><br>

  <b>5 Aug 2021</b><br>
  <span class="easylink">
  I joint JD.com Inc., as a <b><a href="https://campus.jd.com/web/static/forward?to=jd-project-dmt&t=3" ,="" target="_blank">Doctoral Management Trainee</a></b>.
  </span><br><br>

  <b>24 May 2021</b><br>
  <span class="easylink">
  One paper is accepted by <a href="https://multimodalpretraining.github.io/" ,="" target="_blank">ICMR 2021 Workshop</a>, about <b>visual grounding</b>.
  </span><br><br>

  <b>22 May 2021</b><br>
  <span class="easylink">
  <b>I successfully defended my PhD thesis!</b>
  </span><br><br>

  <b>20 Apr 2020</b><br>
  <span class="easylink">
  One paper is accepted by <a href="https://ijcai20.org/" ,="" target="_blank">IJCAI 2020</a>, about <b>video captioning</b>. 
  </span><br><br>

  <b>27 Feb 2020</b><br>
  <span class="easylink">
  One paper is accepted by <a href="https://cvpr2020.thecvf.com/" ,="" target="_blank">CVPR 2020</a>, about <b>image captioning</b>. 
  </span><br><br>

  <b>23 Jul 2019</b><br>
  <span class="easylink">
  One paper is accepted by <a href="http://iccv2019.thecvf.com/" ,="" target="_blank">ICCV 2019</a> (oral), about <b>visual grounding</b>. 
  </span><br><br>

  <b>27 Mar 2019</b><br>
  <span class="easylink">
  Two paper are accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34" ,="" target="_blank">TPAMI</a>, about <b>image captioning</b> and <b>visual grounding</b>. 
  </span><br><br>

  <b>2 Jul 2018</b><br>
  <span class="easylink">
  One paper is accepted by <a href="https://dl.acm.org/doi/proceedings/10.1145/3240508" ,="" target="_blank">ACM MM 2018</a> (oral), about <b>image captioning</b>. 
  </span><br><br>

  <b>5 Mar 2016</b><br>
  <span class="easylink">
  <b>I start my research jourey from USTC!</b>
  </span><br><br>

  <b>Visitors</b><br>
  <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=200&t=tt&d=YJpdTBgS-rLjUaKPDfwQmULKhzMRu0AE32WFXoIqXe4&co=2d78ad&cmo=3acc3a&cmn=ff5353&ct=ffffff"></script>

  </div>

<!-- bio ---------- -->
  <div id="left">
  <table style="background-color:white;">
  <tbody><tr nosave="">
  <td valign="CENTER">
  <img src="./files/profile.jpg" height="190" align="left">
  </td>

  <td valign="CENTER" width="2%">
  </td>

  <td valign="CENTER" halign="LEFT">
  <font size="+0">
  <b><font size="+2">Daqing&nbsp; LIU</font></b>
  <p style="margin-left:0px;">
  <img src="./files/name.png" ,="" height="40">
  </p><p style="margin-left:0px;">
  <b>Research Scientist</b>
  </p><p style="margin-left:0px;">
  JD Explore Academy, <a href="https://corporate.jd.com/contactUs" ,="" target="_blank">JD.com Inc.</a><br>
  Block 2-A-13, JD Building, Beijing, China, 101111<br>
  </p><p style="margin-left:0px;">
  Email:  liudq.ustc at gmail.com<br>
  &bull; <a href="./files/CV_DaqingLiu.pdf" target="_blank">CV</a>
  &bull; <a href="https://scholar.google.com/citations?hl=en&user=TbBfOVEAAAAJ&hl" target="_blank">Google Scholar</a>
  &bull; <a href="https://github.com/daqingliu" target="_blank">GitHub</a>
  </p></font><p><font size="+0">
  </font>
  </p></td>
  </tr>
  </tbody></table>

  <div class="box">
    <p>
    Currently, I am a research scientist at JD Explore Academy (JDEA), JD.com Inc, leaded by Prof. <a href="https://scholar.google.com/citations?user=RwlJNLcAAAAJ&hl=en" target="_blank">Dacheng Tao</a>. Before that, I received my Ph.D. degree in Control Science and Engineering from the University of Science and Technology of China (USTC) in 2021, advised by <a href="https://dblp.org/pers/hd/z/Zha:Zheng=Jun" target="_blank">Prof. Zheng-Jun Zha</a>. From May 2018 to May 2019, I was a visiting student at Nanyang Technological University (NTU), working with <a href="http://www.ntu.edu.sg/home/hanwangzhang/" target="_blank">Asst. Prof. Hanwang Zhang</a>.
    <!-- Before that, I received my B.Eng. degree in Automation from Chang'an University in 2016.<br> -->
    <br>
    <br>
    My primary research interest is Multi-Modality Learning (MML), including multi-modality Understanding, Retrieval, and Generation.<br>
    </p>
  </div>

<!-- Experiences ---------- -->
  <h2 style="CLEAR: both">Experiences</h2>
  <table>
    <tbody><tr>
      <td> <span class="title">Research Scientist</span> <br>
      JD Explore Academy, JD.com Inc., Leader: Prof. <a href="https://scholar.google.com/citations?user=RwlJNLcAAAAJ&hl=en" target="_blank">Dacheng Tao</a> <br>
      Beijing, China, Aug 2021 -- Present <br>
      </td></tr></tbody>
  </table>
  <table>
    <tbody><tr>
      <td> <span class="title">Research Intern</span> <br>
        Nanyang Technological University (NTU), Advisior: Asst. Prof. <a href="http://www.ntu.edu.sg/home/hanwangzhang/" target="_blank">Hanwang Zhang</a> <br>
        Singapore, May 2018 -- May 2019 <br>
      </td></tr></tbody>
  </table>
  <table>
    <tbody><tr>
      <td> <span class="title">Doctor of Engineering</span> <br>
        University of Science and Technology of China (USTC),
        Advisior: Prof. <a href="https://dblp.org/pers/hd/z/Zha:Zheng=Jun" target="_blank">Zheng-Jun Zha</a> <br>
        Hefei, China, Sep 2016 -- Jun 2021 <br>
      </td></tr></tbody>
  </table>
<!--   <table>
    <tbody><tr>
      <td> <span class="title">Bachelor of Engineering</span> <br>
        Chang'an University, major in Automation (Excellent Engineers) <br>
        Xi'an, China, Sep 2012 -- Jun 2016 <br>
      </td></tr></tbody>
  </table> -->

<!-- publications ---------- -->
  <div id="papers">
  <h2>Selected Publications</h2> (<a href="https://scholar.google.com/citations?hl=en&user=TbBfOVEAAAAJ&hl">Google Scholar</a>)
  <br>
  <small><sup>*</sup>: Co-First Authors, <sup>#</sup>: (Co-)supervised Students, <sup>&#9993;</sup>: Corresponding Authors</small>
  <br>
  <br>

  <table>
    <tbody>
    <tr>
      <td class="left"><a href="" target="_blank"><img src="./files/pdf.png" width="25" height="25"><br>pdf</a></td>
      <td><span class="title">Modeling Video as Stochastic Processes for Fine-Grained Video Representation Learning</span>
        <br>Heng Zhang<sup>#</sup>, <b>Daqing Liu</b>, Qi Zheng, Bing Su
      <br> CVPR 2023 (Full, Accept Rate: 25.78%)
    </td>
    </tr>
   </tbody>
  </table>

  <table>
    <tbody>
    <tr>
      <td class="left"><a href="" target="_blank"><img src="./files/pdf.png" width="25" height="25"><br>pdf</a></td>
      <td><span class="title">ESceme: Vision-and-Language Navigation with Episodic Scene Memory</span>
        <br>Qi Zheng<sup>#</sup>, <b>Daqing Liu</b>, Chaoyue Wang, Jing Zhang, Dadong Wang, Dacheng Tao
      <br> arXiv 2023, Comming Soon
    </td>
    </tr>
   </tbody>
  </table>

  <table>
    <tbody>
    <tr>
      <td class="left"><a href="" target="_blank"><img src="./files/pdf.png" width="25" height="25"><br>pdf</a></td>
      <td><span class="title">MMoT: Mixture-of-Modality-Tokens Transformer for Composed Multimodal Conditional Image Synthesis</span>
        <br>Jianbin Zheng<sup>#</sup>, <b>Daqing Liu</b>, Chaoyue Wang, Minghui Hu, Changxing Ding, Dacheng Tao
      <br> arXiv 2023, Comming Soon
    </td>
    </tr>
   </tbody>
  </table>

  <table>
    <tbody>
    <tr>
      <td class="left"><a href="" target="_blank"><img src="./files/pdf.png" width="25" height="25"><br>pdf</a></td>
      <td><span class="title">Tackling the Cross-Modal Retrieval Trilemma with Cross-Modal Indexing</span>
        <br>Heng Zhang<sup>#</sup>, <b>Daqing Liu</b>, Heliang Zheng, Chaoyue Wang, Bing Su
      <br> arXiv 2023, Comming Soon
    </td>
    </tr>
   </tbody>
  </table>

  <table>
    <tbody>
    <tr>
      <td class="left"><a href="" target="_blank"><img src="./files/pdf.png" width="25" height="25"><br>pdf</a></td>
      <td><span class="title">Prefix-Captioning: Efficiently Gluing Pretrained Language and Vision Models for Image Captioning</span>
        <br>Yuanen Zhou<sup>#</sup>, <b>Daqing Liu</b>, Zhenzhen Hu, Depeng Wang, Yi Wang, Meng Wang
      <br> arXiv 2023, Comming Soon
    </td>
    </tr>
   </tbody>
  </table>

  <table>
    <tbody>
    <tr>
      <td class="left"><a href="https://arxiv.org/abs/2211.11190" target="_blank"><img src="./files/pdf.png" width="25" height="25"><br>pdf</a></td>
      <td><span class="title">Cross-Modal Contrastive Learning for Robust Reasoning in VQA</span>
        <br>Qi Zheng<sup>#</sup>, Chaoyue Wang, <b>Daqing Liu</b>, Dadong Wang, Dacheng Tao
      <br> arXiv preprint 2022 &nbsp;&nbsp; <a href="https://github.com/qizhust/cmcl_vqa_pl" target="_blank">Codes</a>
    </td>
    </tr>
   </tbody>
  </table>

  <table>
    <tbody>
    <tr>
      <td class="left"><a href="https://arxiv.org/abs/2206.10207" target="_blank"><img src="./files/pdf.png" width="25" height="25"><br>pdf</a></td>
      <td><span class="title">SemMAE: Semantic-Guided Masking for Learning Masked Autoencoders</span>
        <br>Gang Li, Heliang Zheng, <b>Daqing Liu</b>, Bing Su, Changwen Zheng
      <br> NeurIPS 2022 (Full, Accept Rate: 25.6%) &nbsp;&nbsp; <a href="https://github.com/ucasligang/SemMAE" target="_blank">Codes</a>
    </td>
    </tr>
   </tbody>
  </table>

  <table>
    <tbody>
    <tr>
      <td class="left"><a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548106" target="_blank"><img src="./files/pdf.png" width="25" height="25"><br>pdf</a></td>
      <td><span class="title">Semantically-Consistent Dynamic Blurry Image Generation for Image Deblurring</span>
        <br>Zhaohui Jing, Youjian Zhang, Chaoyue Wang, <b>Daqing Liu</b>, Yong Xia
      <br> ACM MM 2022 (Full, Accept Rate: 27.9%)
    </td>
    </tr>
   </tbody>
  </table>

  <table>
    <tbody>
    <tr>
      <td class="left"><a href="https://arxiv.org/abs/2206.00923" target="_blank"><img src="./files/pdf.png" width="25" height="25"><br>pdf</a></td>
      <td><span class="title">Modeling Image Composition for Complex Scene Generation</span>
        <br>Zuopeng Yang<sup>#</sup>, <b>Daqing Liu<sup>*</sup></b>, Chaoyue Wang, Jie Yang, Dacheng Tao
      <br> CVPR 2022 (Full, Accept Rate:25.3%) &nbsp;&nbsp; <a href="https://github.com/JohnDreamer/TwFA" target="_blank">Codes</a>
    </td>
    </tr>
   </tbody>
  </table>

  <table>
    <tbody>
    <tr>
      <td class="left"><a href="https://arxiv.org/abs/2206.06619" target="_blank"><img src="./files/pdf.png" width="25" height="25"><br>pdf</a></td>
      <td><span class="title">TransVG++: End-to-End Visual Grounding with Language Conditioned Vision Transformer</span>
        <br>Jiajun Deng, Zhengyuan Yang, <b>Daqing Liu</b>, Tianlang Chen, Wengang Zhou, Yanyong Zhang, Houqiang Li, Wanli Ouyang
      <br> arXiv preprint 2022 &nbsp;&nbsp; <a href="https://github.com/djiajunustc/TransVG" target="_blank">Codes</a>
    </td>
    </tr>
   </tbody>
  </table>

  <table>
    <tbody>
    <tr>
      <td class="left"><a href="https://arxiv.org/abs/2201.01984" target="_blank"><img src="./files/pdf.png" width="25" height="25"><br>pdf</a></td>
      <td><span class="title">Compact Bidirectional Transformer for Image Captioning</span>
        <br>Yuanen Zhou<sup>#</sup>, Zhenzhen Hu, <b>Daqing Liu</b>, Huixia Ben, Meng Wang
      <br> arXiv preprint 2022 &nbsp;&nbsp; <a href="https://github.com/yuanezhou/cbtrans" target="_blank">Codes</a>
    </td>
    </tr>
   </tbody>
  </table>

  <table>
    <tbody>
    <tr>
      <td class="left"><a href="./files/mmpt_2021.pdf" target="_blank"><img src="./files/pdf.png" width="25" height="25"><br>pdf</a></td>
      <td><span class="title">Language-Conditioned Region Proposal and Retrieval Network for Referring Expression Comprehension</span>
        <br>Yanwei Xie<sup>#</sup>, <b>Daqing Liu</b>, Xuejin Chen, Zheng-Jun Zha
      <br> ICMR 2021 Workshop on Multi-Modal Pre-Training for Multimedia Understanding
    </td>
    </tr>
   </tbody>
  </table>

  <table>
    <tbody>
    <tr>
      <td class="left"><a href="https://arxiv.org/abs/2007.09049" target="_blank"><img src="./files/pdf.png" width="25" height="25"><br>pdf</a></td>
      <td><span class="title">Learning to Discretely Compose Reasoning Module Networks for Video Captioning</span> 
        <br>Ganchao Tan<sup>#</sup>, <b>Daqing Liu<sup>*</sup></b>, Meng Wang, Zheng-Jun Zha 
      <br> IJCAI 2020 (Oral, Accept Rate: 12.6%) &nbsp;&nbsp; <a href="https://github.com/tgc1997/RMN" target="_blank">Codes</a>
    </td> 
    </tr>
   </tbody>
  </table>

  <table>
    <tbody>
    <tr>
      <td class="left"><a href="https://arxiv.org/abs/2004.00390" target="_blank"><img src="./files/pdf.png" width="25" height="25"><br>pdf</a></td>
      <td><span class="title">More Grounded Image Captioning by Distilling Image-Text Matching Model</span> 
        <br>Yuanen Zhou<sup>#</sup>, Meng Wang, <b>Daqing Liu</b>, Zhenzhen Hu, Hanwang Zhang
      <br> CVPR 2020 (Full, Accept Rate: 22%) &nbsp;&nbsp; <a href="https://github.com/YuanEZhou/Grounded-Image-Captioning" target="_blank">Codes</a>
    </td> 
    </tr>
   </tbody>
  </table>

  <table>
    <tbody>
    <tr>
      <td class="left"><a href="https://arxiv.org/abs/1906.03561" target="_blank"><img src="./files/pdf.png" width="25" height="25"><br>pdf</a></td>
      <td><span class="title">Joint Visual Grounding with Language Scene Graphs</span> 
        <br><b>Daqing Liu</b>, Hanwang Zhang, Zheng-Jun Zha, Meng Wang, Qianru Sun
      <br> arXiv preprint 2019
    </td> 
    </tr>
   </tbody>
  </table>

  <table>
    <tbody>
    <tr>
      <td class="left"><a href="https://arxiv.org/abs/1812.03299" target="_blank"><img src="./files/pdf.png" width="25" height="25"><br>pdf</a></td>
      <td><span class="title">Learning to Assemble Neural Module Tree Networks for Visual Grounding</span> 
        <br><b>Daqing Liu</b>, Hanwang Zhang, Zheng-Jun Zha, Feng Wu
      <br> ICCV 2019 (Oral, Accept Rate: 4.3%) &nbsp;&nbsp; <a href="https://github.com/daqingliu/NMTree" target="_blank">Codes</a> &nbsp;&nbsp; <a href="https://youtu.be/oFDF1yT0T-4?t=3574" target="_blank">Talk</a>
    </td> 
    </tr>
   </tbody>
  </table>

  <table>
    <tbody>
    <tr>
      <td class="left"><a href="https://arxiv.org/abs/1906.02365" target="_blank"><img src="./files/pdf.png" width="25" height="25"><br>pdf</a></td>
      <td><span class="title">Context-Aware Visual Policy Network for Fine-Grained Image Captioning</span> 
        <br>Zheng-Jun Zha, <b>Daqing Liu<sup>&#9993;</sup></b>, Hanwang Zhang, Yongdong Zhang, Feng Wu
      <br>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI, Impact Factor: 17.730) &nbsp;&nbsp; <a href="https://github.com/daqingliu/CAVP" target="_blank">Codes</a>
    </td> 
    </tr>
   </tbody>
  </table>

  <table>
    <tbody>
    <tr>
      <td class="left"><a href="https://arxiv.org/abs/1906.01784" target="_blank"><img src="./files/pdf.png" width="25" height="25"><br>pdf</a></td>
      <td><span class="title">Learning to Compose and Reason with Language Tree Structures for Visual Grounding</span> 
        <br>Richang Hong, <b>Daqing Liu<sup>&#9993;</sup></b>, Xiaoyu Mo, Xiangnan He, Hanwang Zhang</b> 
      <br>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI, Impact Factor: 17.730)
    </td> 
    </tr>
   </tbody>
  </table>

  <table>
    <tbody>
    <tr>
      <td class="left"><a href="https://arxiv.org/abs/1808.05864" target="_blank"><img src="./files/pdf.png" width="25" height="25"><br>pdf</a></td>
      <td><span class="title">Context-Aware Visual Policy Network for Sequence-Level Image Captioning</span> 
        <br><b>Daqing Liu</b>, Zheng-Jun Zha, Hanwang Zhang, Yongdong Zhang, Feng Wu
      <br>ACM MM 2018 (Oral, Accept Rate: 8.5%) &nbsp;&nbsp; <a href="https://github.com/daqingliu/CAVP" target="_blank">Codes</a>
    </td> 
    </tr>
   </tbody>
  </table>

<!-- Services ----------
<h2 style="CLEAR: both;">Services</h2>
<table><tbody><tr><td>
  <span class="title">Journal Reviewer</span>: <br>
   IEEE TMM, IEEE TNNLS, IEEE TOMM, Neurocomputing
</td></tr></tbody></table>

<table><tbody><tr><td>
  <span class="title">PC Member of Conferences</span>: <br>
  <b>2022</b>: CVPR, ECCV, AAAI <br>
  <b>2021</b>: CVPR, ICCV, AAAI <br>
</td></tr></tbody></table>
-->

<!-- honors ---------- -->
<h2 style="CLEAR: both;">Honors</h2>

<table><tbody><tr><td>
  <span class="title">Outstanding Graduates, &nbsp;&nbsp;Jun 2021</span> &nbsp;&nbsp;
  <br> - University of Science and Technology of China
</td></tr></tbody></table>

<table><tbody><tr><td>
  <span class="title">Scholarship Award, &nbsp;&nbsp;Nov 2019</span> &nbsp;&nbsp;
  <br> - Huawei Technologies Co., Ltd.
</td></tr></tbody></table>

<table><tbody><tr><td>
  <span class="title">Scholarship Award, &nbsp;&nbsp;Dec 2018</span> &nbsp;&nbsp;
  <br> - China Aerospace and Technology Corporation
</td></tr></tbody></table>

<table><tbody><tr><td>
  <span class="title">National Second Prize, &nbsp;&nbsp;Nov 2015</span> &nbsp;&nbsp;
  <br> - The 14th "Challenge Cup" National University Extracurricular Scientific Works Competition
</td></tr></tbody></table>

<table><tbody><tr><td>
  <span class="title">Pivot of Merit Student, &nbsp;&nbsp;Nov 2014</span> &nbsp;&nbsp;
  <br> - Chang'an University
</td></tr></tbody></table>

<table><tbody><tr><td>
  <span class="title">Winning Prize, &nbsp;&nbsp;Jul 2014</span> &nbsp;&nbsp;
  <br> - The 9th "Freescale Cup" National University Intelligent Car Competition
</td></tr></tbody></table>

<br>
<p>Last update: 28 Feb, 2023. Webpage template borrows from <a href="http://staff.ustc.edu.cn/~hexn/">here</a>.</p>

</div>
</div>


</body></html>